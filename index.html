<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="3D Part Segmentation via Geometric Aggregation of 2D Visual Features">
  <meta name="keywords" content="COPS, DINO, point cloud, point cloud understanding, 3D part segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>3D Part Segmentation via Geometric Aggregation of 2D Visual Features</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">3D Part Segmentation via Geometric Aggregation of 2D Visual Features</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.at/citations?user=BEIogS4AAAAJ">Marco Garosi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.at/citations?user=E_QzprYAAAAJ">Riccardo Tedoldi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=dT1N2IUAAAAJ">Davide Boscaini</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.at/citations?user=bqTPA8kAAAAJ">Massimiliano Mancini</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=BQ7li6AAAAAJ">Fabio Poiesi</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.it/citations?user=stFCYOAAAAAJ">Nicu Sebe</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Trento,</span>
            <span class="author-block"><sup>2</sup>TeV - Fondazione Bruno Kessler</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/marco-garosi/COPS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Teaser. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">          
          <img src="./static/images/teaser.png" alt="COPS teaser"/>
          <p>
            
            The quality of the parts' description heavily influences the part segmentation performance of methods based on vision-language models.
            For instance, the performance of PointCLIPv2 (left), deteriorates rapidly when replacing the default textual prompt with <i>e.g.</i>, a GPT-generated description, the template "This is a depth image of an airplane's (part)", or simply using part names.
            In contrast, our pipeline (center) produces more accurate segmentations by disentangling part decomposition from part classification.
            The improvement is evident when utilising the same CLIP visual features as PointCLIPv2 (top) and further increases when using DINOv2 features (bottom), the default choice of COPS.
            <b>COPS</b> generates more <b>uniform segments with sharper boundaries</b>, resulting in higher segmentation quality.
          </p>
        </div>
      </div>
    </div>
    <!--/ Teaser. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">          
          <p>
            Supervised 3D part segmentation models are tailored to a predetermined closed set of objects and parts. This restriction limits their transferability to open-set, real-world scenarios.
            Recent works explored vision-language models (VLMs) as a promising alternative where parts are identified through multi-view rendering and prompting.
          </p>
          <p>
            However, naively applying VLMs in this setting brings many of their disadvantages (<i>e.g.</i> meticulously crafting prompts) and fails to leverage the 3D geometric structure of the object.
            To address these limitations, we propose COPS, a <b>co</b>mprehensive model for <b>p</b>arts <b>s</b>egmentation that blends the semantics extracted from visual concepts and 3D geometry to identify the parts effectively.
            It renders a point cloud from multiple viewpoints, extracts 2D features, projects them back to 3D, and introduces geometric information to ensure spatial and semantic consistency with a new geometric module. Lastly, it clusters points into parts and labels them.
          </p>
          <p>
            We demonstrate that COPS is an efficient, scalable, and simple yet effective method for achieving zero-shot SOTA performance on five datasets, encompassing both synthetic and real-world data, texture-less and coloured objects, and both rigid and non-rigid shapes.
            The code is openly available.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">     
          <img src="./static/images/arch.png" alt="COPS architecture"/>
          <p>
            Overview of COPS's feature extractor.
            \(\Phi\) (top) extracts point-level features by (i) rendering multiple views of the object, (ii) processing them with DINOv2, (iii) lifting them in 3D.
          </p>
          <p>
            The Geometric Feature Aggregation module (GFA, bottom) further refines these features by extracting superpoints (blue points in the second row) and their neighbouring points (red points in the second row) to obtain spatially consistent centroids.
            These centroids are used to perform spatial- and semantic-consistent feature aggregation, ensuring that the features are both locally consistent and similar across large distances when describing the same part (<i>e.g.</i> the armrest).
          </p>
          <p>
            COPS performs feature extraction from 3D point clouds using DINOv2, which has been developed for image processing tasks and trained on unrelated web-scale data.
            Following common practice in the literature, we address the dimensionality gap by projecting 3D objects onto a collection of 2D images rendered from various viewpoints.
          </p>
          <p>
            2D images are input into a pre-trained frozen DINOv2 base model to generate patch-level feature maps, which are then upsampled via bicubic interpolations to match the original rendering size. Pixel-level features are mapped back to 3D using correspondences between 2D pixels and 3D points computed by the rendering tool. 
            Features from multiple viewpoints belonging to the same part of the 3D shape are clustered using a geometrically informed fusion mechanism to create a cohesive representation. 
            These versatile 3D features can be applied to various point-level tasks related to 3D object understanding without additional training.
            We then perform zero-shot 3D point cloud part segmentation, by leveraging CLIP's predictions to align the clustered parts with semantic labels.
          </p>
        </div>
      </div>
    </div>

    <!-- Steps -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">How COPS processes the input</h2>
        <div class="content has-text-justified">
          <img src="./static/images/qualitatives.png" alt="Qualitative results on ShapeNetPart"/>
          <p>
            Detailed visualisation of the steps required by COPS.
            From left to right:
            <ul>
              <li>input point cloud,</li>
              <li>intermediate features obtained by 3D-lifting DINOv2 features,</li>
              <li>final features obtained with GFA,</li>
              <li>part decomposition obtained via feature clustering (colours are not informative because cluster labels are not semantic),</li>
              <li>PointCLIPv2 predictions,</li>
              <li>COPS predictions,</li>
              <li>and ground-truth segmentation.</li>
            </ul>
            By disentangling part decomposition (fourth column) from semantic label assignment, COPS can leverage noisy PointCLIPv2 predictions (fifth column) to produce accurate segmentations (sixth column).
          </p>
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Qualitative results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative results</h2>
        <div class="content has-text-justified">     
          <h3 class="title is-8">ShapeNetPart</h3>
          <img src="./static/images/qual_shapenetpart.png" alt="Qualitative results on ScanObject-NN"/>

          <h3 class="title is-8">ScanObject-NN</h3>
          <img src="./static/images/qual_sonn.png" alt="Qualitative results on ScanObject-NN"/>

          <h3 class="title is-8">FAUST</h3>
          <img src="./static/images/qual_faust.png" alt="Qualitative results on ScanObject-NN"/>
        </div>
      </div>
    </div>
    <!--/ Qualitative results. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{garosi2025cops,
  author    = {Garosi, Marco and Tedoldi, Riccardo and Boscaini, Davide and Mancini, Massimiliano and Poiesi, Fabio and Sebe, Nicu},
  title     = {3D Part Segmentation via Geometric Aggregation of 2D Visual Features},
  journal   = {WACV},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/marco-garosi/COPS.github.io" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The webpage template is from
            <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
